---
title: "DNR Nearshore Sampling Site Cluster Analysis"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(targets)
library(tidyverse)
library(plotly)
library(DT)
library(leaflet)

dnr <- tar_read(dnr, store = "../_targets") %>% 
  select(-c(nh3, doc, do,  chl_field, depth, source, type)) %>% 
               filter(str_detect(site, "site")) %>% 
               mutate(site = str_split_i(site, " ", 2),
                      site = as.factor(site))

dnr_sd_date <- dnr %>% 
  select(-c(latitude, longitude)) %>% 
  group_by(date) %>% 
  summarise(across(where(is.numeric), ~sd(., na.rm=TRUE)))
dnr_sd_date_means <- dnr_sd_date %>% 
  summarise(across(-date, ~mean(., na.rm=TRUE)))

dnr_sd_site <- dnr %>% 
  mutate(site = str_c(site, year(date), sep = "-")) %>% 
  select(-c(latitude, longitude, date)) %>% 
  group_by(site) %>% 
  summarise(across(where(is.numeric), ~sd(., na.rm=TRUE)))
dnr_sd_site_means <- dnr_sd_site %>% 
  summarise(across(-site, ~mean(., na.rm=TRUE)))

dnr_sd_comp <- bind_rows(dnr_sd_date_means, dnr_sd_site_means) 
ratios <- dnr_sd_comp[2,] / dnr_sd_comp[1,]
dnr_sd_comp <- bind_rows(dnr_sd_comp, ratios) %>% 
  mutate(across(everything(), ~round(., 5)),
         type = c("date", "site", "site / date")) %>% 
  relocate(type)



StatChull <- ggproto("StatChull", Stat,
  compute_group = function(data, scales) {
    data[chull(data$x, data$y), , drop = FALSE]
  },
  
  required_aes = c("x", "y")
)
stat_chull <- function(mapping = NULL, data = NULL, geom = "polygon",
                       position = "identity", na.rm = FALSE, show.legend = NA, 
                       inherit.aes = TRUE, ...) {
  layer(
    stat = StatChull, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )
}

.get_withinSS <- function(d, cluster){
  d <- as.dist(d)
  cn <- max(cluster)
  clusterf <- as.factor(cluster)
  clusterl <- levels(clusterf)
  cnn <- length(clusterl)
  
  cwn <- cn
  # Compute total within sum of square
  dmat <- as.matrix(d)
  within.cluster.ss <- 0
  for (i in 1:cn) {
    cluster.size <- sum(cluster == i)
    di <- as.dist(dmat[cluster == i, cluster == i])
    within.cluster.ss <- within.cluster.ss + sum(di^2)/cluster.size
  }
  within.cluster.ss
}

opt_clus <- function(x, kmax){
  if (is.data.frame(x)) x <- as.matrix(x)
  diss <- dist(x)
  
  v <- rep(0, kmax)
  
  for(i in 1:kmax){
    clust <- kmeans(x, i, nstart = 100)
    v[i] <- .get_withinSS(diss, clust$cluster)
  }
  
  df <- data.frame(clusters = as.factor(1:kmax), y = v, stringsAsFactors = TRUE)
  
  ggplot(data = df, aes(x = clusters, y = y, group = 1)) +
    geom_point() + 
    geom_line() +
    xlab("Number of clusters k") +
    ylab("Total Within Sum of Square") +
    ggtitle("Optimal number of clusters") +
    theme_classic()
  
}
# Function to calculate Euclidean distance
euclidean_distance <- function(df1, df2) {
  sqrt(sum((df1 - df2) ^ 2))
}

# Function to evaluate subsets
evaluate_subsets <- function(data, subset_size) {
  n <- nrow(data)
  all_subsets <- combn(n, subset_size, simplify = FALSE)
  
  # Calculate the mean of the entire data frame
  overall_mean <- colMeans(data)
  
  # Initialize a list to store results
  results <- list()
  
  for (subset_indices in all_subsets) {
    subset_data <- data[subset_indices, ]
    subset_mean <- colMeans(subset_data)
    
    # Calculate the distance from the overall mean
    distance <- euclidean_distance(overall_mean, subset_mean)
    
    # Store the indices as a single string and the distance in the results list
    results[[length(results) + 1]] <- data.frame(
      indices = paste(subset_indices, collapse = ", "),
      distance = distance
    )
  }
  
  # Combine all results into a single data frame
  results_df <- do.call(rbind, results)
  return(results_df)
}
```

## Quick comparison of space vs time

This table shows average standard deviations with sampling dates, within site-years (ie. site 1 in 2019 and site 1 in 2021 are different sites), and the ratio between the two. Row 3 being above 1 means that within site variation (temporal variation) is greater than within date variation (spatial variation). Across all parameters we have greater temporal variation than spatial variation, which I find to be a strong argument in favor of sampling fewer sites on more days. 
```{r spacetime, echo=FALSE}
datatable(dnr_sd_comp)
```

Now I'm going to see if looking at how sites are clustered can help us figure out which sites to sample more often. I'll be using 8 groups throughout (3 for Northland College Sites), as I'm assuming that's the number of sites we need to narrow down to.

## Sites 1-15, 2019-2024 {.tabset}

### Site Means
Here's how 8 clusters breaks down when looking at *average* values at each site:
```{r plot mean, echo=FALSE}
prep_base <- dnr %>% 
  filter(!(site %in% c("16", "17", "18", "19", "20"))) %>% 
               group_by(site) %>% 
               summarise(across(where(is.numeric), ~mean(., na.rm = TRUE))) 
scale_base <- prep_base %>% 
  select(-c(latitude, longitude)) %>% 
               column_to_rownames("site") %>% 
               scale()
pca_base <- prcomp(scale_base)
pca_base_load <- data.frame(pca_base$rotation)
pca_base_var <- (pca_base$sdev^2) / sum(pca_base$sdev^2)
kmean_base <- kmeans(scale_base, 8, nstart = 100)
kmean_base_df <- data.frame(site = names(kmean_base$cluster), cluster = as.numeric(kmean_base$cluster))

pca_base_df <- data.frame(pca_base$x) %>% 
  rownames_to_column(var = "site") %>% 
  arrange(site) %>%
  left_join(kmean_base_df) %>%
  mutate(across(c(site, cluster), as.factor))
ggplot(data = pca_base_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_segment(data = pca_base_load,
               aes(x = 0, y = 0, xend = PC1 * 2, yend = PC2 * 2),
               arrow = arrow(length = unit(0.1, "inches")),
               color = "darkgrey") +
  geom_text(data = pca_base_load,
            aes(x = PC1 * 2, y = PC2 * 2, label = rownames(pca_base_load)),
            vjust = 1, hjust = 1, color = "black") +
  stat_chull(fill = NA) +
  geom_label(aes(label = site)) +
  labs(x = paste0("PC", 1, " (", round(pca_base_var[1] * 100, 1), "%)"),
       y = paste0("PC", 2, " (", round(pca_base_var[2] * 100, 1), "%)"))

```


In order to help decide which sites to include from clusters with more than one site, I calculated the euclidean distance between averages of principal components for combinations of 8 sites and the full 15 sites. I only looked at combinations that fit the cluster pattern, so all combinations had to include sites 1, 2, 4, and 9; one of 3 or 8; one of 5,6,7; one of 10,11,12; and one of 13,14,15. The table below shows the top 10 combinations; a smaller distance values means it is more similar to the full group of sites.
```{r dist mean, echo = FALSE}
combo_dist_base <- evaluate_subsets(scale_base, subset_size = 8) %>% 
  mutate(distance = round(distance, 4)) %>% 
  arrange(distance) 

combo_dist <- combo_dist_base %>% 
  filter(
                       str_detect(indices, "(?<!1)1,") &
                       str_detect(indices, "(?<!1)2,") &
                         (str_detect(indices, "(?<!1)3,") | str_detect(indices, "8")) &
                         str_detect(indices, "(?<!1)4,") &
                         (str_detect(indices, "(?<!1)5,") | str_detect(indices, "6") | str_detect(indices, "7")) &
                         str_detect(indices, "9") &
                         (str_detect(indices, "10") | str_detect(indices, "11") | str_detect(indices, "12")) &
                         (str_detect(indices, "13") | str_detect(indices, "14") | str_detect(indices, "15"))) %>% 
  head(10)

# index_counts <- combo_dist %>%
#   mutate(indices = str_split(indices, ", ")) %>%
#   unnest(indices) %>%
#   group_by(indices) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   arrange(desc(count)) %>% 
#   rename(site = indices)

datatable(combo_dist, rownames = FALSE)
```

Assuming that 1, 2, 4, and 9 are being sampled, this tables indicates that 8 is a better fit than 3 (8 is in the top 4 combos and 7 of the top 10), 5 is slightly better than 6 or 7 but not by much (7 appears most frequently, 5 is in the top 2 combos), 11 and 12 are better fits than 10; and 15 is better than 13 or 14.

For comparison, here are the top 10 closest combinations overall, ignoring the cluster pattern. Picking one of these combinations would also make a lot of sense, particularly if there is good overlap with clusters.
```{r dist mean 2, echo = FALSE}
datatable(head(combo_dist_base,10), rownames = FALSE)
```

### Site Medians
Alternatively, here's how 8 clusters breaks down when looking at *median* values at each site:
```{r plot median, echo=FALSE}
prep_base <- dnr %>% 
  filter(!(site %in% c("16", "17", "18", "19", "20"))) %>% 
               group_by(site) %>% 
               summarise(across(where(is.numeric), ~median(., na.rm = TRUE))) 
scale_base <- prep_base %>% 
  select(-c(latitude, longitude)) %>% 
               column_to_rownames("site") %>% 
               scale()
pca_base <- prcomp(scale_base)
pca_base_load <- data.frame(pca_base$rotation)
pca_base_var <- (pca_base$sdev^2) / sum(pca_base$sdev^2)
kmean_base <- kmeans(scale_base, 8, nstart = 100)
kmean_base_df <- data.frame(site = names(kmean_base$cluster), cluster = as.numeric(kmean_base$cluster))

pca_base_df <- data.frame(pca_base$x) %>% 
  rownames_to_column(var = "site") %>% 
  arrange(site) %>%
  left_join(kmean_base_df) %>%
  mutate(across(c(site, cluster), as.factor))
ggplot(data = pca_base_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_segment(data = pca_base_load,
               aes(x = 0, y = 0, xend = PC1 * 2, yend = PC2 * 2),
               arrow = arrow(length = unit(0.1, "inches")),
               color = "darkgrey") +
  geom_text(data = pca_base_load,
            aes(x = PC1 * 2, y = PC2 * 2, label = rownames(pca_base_load)),
            vjust = 1, hjust = 1, color = "black") +
  stat_chull(fill = NA) +
  geom_label(aes(label = site)) +
  labs(x = paste0("PC", 1, " (", round(pca_base_var[1] * 100, 1), "%)"),
       y = paste0("PC", 2, " (", round(pca_base_var[2] * 100, 1), "%)"))



```


Now we're using median site values for the distance calculations, so all combinations had to include sites 1, 6, 7, and 8; one of 2 or 4; one of 3 or 5; one of 9,10,11,12; and one of 13,14,15. The table below shows the top 10 combinations; a smaller distance values means it is more similar to the full group of sites.
```{r dist median, echo = FALSE}
combo_dist_base <- evaluate_subsets(scale_base, subset_size = 8) %>% 
  mutate(distance = round(distance, 4)) %>% 
  arrange(distance) 

combo_dist <- combo_dist_base %>%
  filter(
                       str_detect(indices, "(?<!1)1,") &
                       (str_detect(indices, "(?<!1)2,") | str_detect(indices, "(?<!1)4,")) &
                         (str_detect(indices, "(?<!1)3,") | str_detect(indices, "(?<!1)5,")) &
                         str_detect(indices, "6") &
                         str_detect(indices, "7") &
                         str_detect(indices, "8") &
                         (str_detect(indices, "9") | str_detect(indices, "10") | str_detect(indices, "11") | str_detect(indices, "12")) &
                         (str_detect(indices, "13") | str_detect(indices, "14") | str_detect(indices, "15"))) %>% 
  head(10)

# index_counts <- combo_dist %>%
#   mutate(indices = str_split(indices, ", ")) %>%
#   unnest(indices) %>%
#   group_by(indices) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   arrange(desc(count)) %>% 
#   rename(site = indices)

datatable(combo_dist, rownames = FALSE)
```

Assuming that 1, 6, 7, and 8 are being sampled, this tables indicates that 3 is a better fit than 5, 4 is better than 2, 12 is slightly better than 9 and 11 and much better than 10; and 13 is better than 14 or 15.

For comparison, here are the top 10 closest combinations overall, ignoring the cluster pattern. Again, picking one of these combinations would also make a lot of sense, particularly if there is good overlap with clusters.
```{r dist median 2, echo = FALSE}
datatable(head(combo_dist_base,10), rownames = FALSE)
```

### Site Standard Deviations
Finally, here's how 8 clusters breaks down when looking at the *standard deviation* of values at each site:
```{r plot sd, echo=FALSE}
prep_base <- dnr %>% 
  filter(!(site %in% c("16", "17", "18", "19", "20"))) %>% 
               group_by(site) %>% 
               summarise(across(where(is.numeric), ~sd(., na.rm = TRUE))) 
scale_base <- prep_base %>% 
  select(-c(latitude, longitude)) %>% 
               column_to_rownames("site") %>% 
               scale()
pca_base <- prcomp(scale_base)
pca_base_load <- data.frame(pca_base$rotation)
pca_base_var <- (pca_base$sdev^2) / sum(pca_base$sdev^2)
kmean_base <- kmeans(scale_base, 8, nstart = 100)
kmean_base_df <- data.frame(site = names(kmean_base$cluster), cluster = as.numeric(kmean_base$cluster))

pca_base_df <- data.frame(pca_base$x) %>% 
  rownames_to_column(var = "site") %>% 
  arrange(site) %>%
  left_join(kmean_base_df) %>%
  mutate(across(c(site, cluster), as.factor))
ggplot(data = pca_base_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_segment(data = pca_base_load,
               aes(x = 0, y = 0, xend = PC1 * 2, yend = PC2 * 2),
               arrow = arrow(length = unit(0.1, "inches")),
               color = "darkgrey") +
  geom_text(data = pca_base_load,
            aes(x = PC1 * 2, y = PC2 * 2, label = rownames(pca_base_load)),
            vjust = 1, hjust = 1, color = "black") +
  stat_chull(fill = NA) +
  geom_label(aes(label = site)) +
  labs(x = paste0("PC", 1, " (", round(pca_base_var[1] * 100, 1), "%)"),
       y = paste0("PC", 2, " (", round(pca_base_var[2] * 100, 1), "%)"))



```


Using standard deviations, all combinations had to include sites 1, 2, 4, 7, and 9; one of 3,5,6,8; one of 10,11,12; and one of 13,14,15. The table below shows the top 10 combinations; a smaller distance values means it is more similar to the full group of sites.
```{r dist sd, echo = FALSE}
combo_dist_base <- evaluate_subsets(scale_base, subset_size = 8) %>% 
  mutate(distance = round(distance, 4)) %>% 
  arrange(distance)  

combo_dist <- combo_dist_base %>%
  filter(
                       str_detect(indices, "(?<!1)1,") &
                       str_detect(indices, "(?<!1)2,") &
                         (str_detect(indices, "(?<!1)3,") | str_detect(indices, "8") | str_detect(indices, "(?<!1)5,") | str_detect(indices, "6")) &
                         str_detect(indices, "(?<!1)4,") &
                         str_detect(indices, "7") &
                         str_detect(indices, "9") &
                         (str_detect(indices, "10") | str_detect(indices, "11") | str_detect(indices, "12")) &
                         (str_detect(indices, "13") | str_detect(indices, "14") | str_detect(indices, "15"))) %>% 
  head(10)

# index_counts <- combo_dist %>%
#   mutate(indices = str_split(indices, ", ")) %>%
#   unnest(indices) %>%
#   group_by(indices) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   arrange(desc(count)) %>% 
#   rename(site = indices)

datatable(combo_dist, rownames = FALSE)
```

Assuming that 1, 2, 4, 7, and 9 are being sampled, this tables indicates that 8 and 5 are better than 6 or 3, 10 or 11 are better than 12; and 14 or 15 are better than 13.

For comparison, here are the top 10 closest combinations overall, ignoring the cluster pattern. Again, picking one of these combinations would also make a lot of sense, particularly if there is good overlap with clusters.
```{r dist sd 2, echo = FALSE}
datatable(head(combo_dist_base,10), rownames = FALSE)
```


## Takeaways for WDNR sites 1-15

I didn't dive very deep into how the clusters compare with each other (describing spatial variation), but I plan on doing so in the next few weeks. I wanted this to be focused on narrowing down and picking a subset of sites.

There are a lot of ways to subset the 15 sites, and most of them can be justified. So the good news is that it's hard to make a wrong decision. I'm inclined to give more weight to the clusters based on mean and standard deviation, since they provide a better estimate of the diversity of conditions at each site, rather than each site's baseline. I think it's ok to include a couple sites with similar median values for some parameters if those sites experience different ranges of conditions.

In general, there's a clear pattern where we see 2 clusters in the East (10-15) and 6 clusters in the West (1-9), which I believe makes sense given the larger number and size of tributaries in the West. 

If it were me, I would choose 1, 2, 4, 7, 8, 9, 11, and 14. I think it's really hard to go wrong picking between 13-15, so if there's a strong historical reason to pick one of those I would go with it. This combination aligns with the PCA clusters based on site averages and has a pretty even geographic spread:

```{r map 8, echo = FALSE}
prep_base <- dnr %>% 
  filter(!(site %in% c("16", "17", "18", "19", "20"))) %>% 
               group_by(site) %>% 
               summarise(across(where(is.numeric), ~mean(., na.rm = TRUE))) 
scale_base <- prep_base %>% 
  select(-c(latitude, longitude)) %>% 
               column_to_rownames("site") %>% 
               scale()
pca_base <- prcomp(scale_base)
pca_base_load <- data.frame(pca_base$rotation)
pca_base_var <- (pca_base$sdev^2) / sum(pca_base$sdev^2)
kmean_base <- kmeans(scale_base, 8, nstart = 100)
kmean_base_df <- data.frame(site = names(kmean_base$cluster), cluster = as.numeric(kmean_base$cluster))
prep_base2 <- dnr %>% 
  filter((site %in% c("1", "2", "4", "7", "8", "9", "11", "14"))) %>% 
               group_by(site) %>% 
               summarise(across(where(is.numeric), ~mean(., na.rm = TRUE))) 
map_data <- left_join(prep_base2, kmean_base_df)
pal <- colorFactor(
      palette="viridis",
      domain=map_data$cluster
    )

    leaflet() %>%
      addProviderTiles('Esri.WorldStreetMap') %>%
      addCircleMarkers(
        data = map_data,
        lat = ~latitude,
        lng = ~longitude,
        label = ~str_c(site, ", cluster #", cluster, sep = ""),
        fillColor = ~pal(cluster),
        color="black",
        weight =.5,
        opacity=1,
        fillOpacity=1,
      )

```


Here's all 15, colored by the same (site mean) clusters
```{r map base, echo = FALSE}

map_data <- left_join(prep_base, kmean_base_df)
pal <- colorFactor(
      palette="viridis",
      domain=map_data$cluster
    )

    leaflet() %>%
      addProviderTiles('Esri.WorldStreetMap') %>%
      addCircleMarkers(
        data = map_data,
        lat = ~latitude,
        lng = ~longitude,
        label = ~str_c(site, ", cluster #", cluster, sep = ""),
        fillColor = ~pal(cluster),
        color="black",
        weight =.5,
        opacity=1,
        fillOpacity=1,
      )

```


This table compares the average value of each parameter between my proposed subset and the full group of sites:
```{R subset comp, echo = FALSE}
dnr2 <- dnr %>% 
  filter(!(site %in% c("16", "17", "18", "19", "20")))

dnr_mean_site <- dnr2 %>% 
  mutate(site = str_c(site, year(date), sep = "-")) %>% 
  select(-c(latitude, longitude, date)) %>% 
  #group_by(site) %>% 
  summarise(across(where(is.numeric), ~mean(., na.rm=TRUE)))
subset_mean_site <- dnr2 %>% 
  filter(site %in% c("1", "2", "4", "7", "8", "9", "11", "14")) %>% 
  mutate(site = str_c(site, year(date), sep = "-")) %>% 
  select(-c(latitude, longitude, date)) %>% 
  summarise(across(where(is.numeric), ~mean(., na.rm=TRUE)))

dnr_comp <- bind_rows(dnr_mean_site, subset_mean_site)
ratios <- round((dnr_comp[2,] - dnr_comp[1,]) / dnr_comp[1,] * 100, 1)
dnr_comp <- bind_rows(dnr_comp, ratios) %>% 
  mutate(across(everything(), ~round(., 5)),
         type = c("full", "subset", "subset % diff")) %>% 
  relocate(type)
```

The subset has notably higher TSS and turbidity, with somewhat higher TP and chlorophyll. This is because we dropped proportionally more Eastern sites, which also have some of the lowest TSS, turbidity, TP, and chlorophyll. See site means: 

```{r means, echo = FALSE}
dnr2 %>% 
  select(-c(latitude, longitude, date)) %>% 
  group_by(site) %>% 
  summarise(across(where(is.numeric), ~mean(., na.rm=TRUE))) %>% 
  mutate(across(-site, ~round(.,4))) %>% 
  datatable()
```

```{r group_plots, echo=FALSE, fig.width=3, fig.height=5}
#Here are some plots comparing variables across the 8 clusters (with a few high outliers dropped for tss, tp, tn)
# dnr_clusters <- dnr %>%
#   filter(str_detect(site, "site")) %>%
#   mutate(site = str_split_i(site, " ", 2)) %>%
#   left_join(kmean_base_df) %>% 
#   mutate(cluster = as.factor(cluster)) %>% 
#   filter(!is.na(cluster))
# 
# ggplot(data = dnr_clusters, aes(x = cluster, y = chl, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# dnr_clusters %>%
#   filter(tss < 50) %>%
#   ggplot(aes(x = cluster, y = tss, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# dnr_clusters %>%
#   filter(tp < 0.05) %>%
#   ggplot(aes(x = cluster, y = tp, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# ggplot(data = dnr_clusters, aes(x = cluster, y = po4, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# dnr_clusters %>%
#   filter(tn < 1) %>%
#   ggplot(aes(x = cluster, y = tn, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# ggplot(data = dnr_clusters, aes(x = cluster, y = no3, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# ggplot(data = dnr_clusters, aes(x = cluster, y = temp, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# ggplot(data = dnr_clusters, aes(x = cluster, y = cond, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")
# ggplot(data = dnr_clusters, aes(x = cluster, y = do_sat, color = cluster)) +
#   geom_boxplot() +
#   guides(color="none")

```





## Northand College Sites 16-20

Not sure if we need to drop sites here, but thought we might for financial reasons if we're sampling more often. I'm going with 3 clusters, but that's easy to change.

Here's how 3 clusters breaks down when looking at *average* values at each site:
```{r plot mean nc, echo=FALSE}
prep_base <- dnr %>% 
  filter((site %in% c("16", "17", "18", "19", "20"))) %>% 
               group_by(site) %>% 
               summarise(across(where(is.numeric), ~mean(., na.rm = TRUE))) 
scale_base <- prep_base %>% 
  select(-c(latitude, longitude)) %>% 
               column_to_rownames("site") %>% 
               scale()
pca_base <- prcomp(scale_base)
pca_base_load <- data.frame(pca_base$rotation)
pca_base_var <- (pca_base$sdev^2) / sum(pca_base$sdev^2)
kmean_base <- kmeans(scale_base, 3, nstart = 100)
kmean_base_df <- data.frame(site = names(kmean_base$cluster), cluster = as.numeric(kmean_base$cluster))

pca_base_df <- data.frame(pca_base$x) %>% 
  rownames_to_column(var = "site") %>% 
  arrange(site) %>%
  left_join(kmean_base_df) %>%
  mutate(across(c(site, cluster), as.factor))
ggplot(data = pca_base_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_segment(data = pca_base_load,
               aes(x = 0, y = 0, xend = PC1 * 2, yend = PC2 * 2),
               arrow = arrow(length = unit(0.1, "inches")),
               color = "darkgrey") +
  geom_text(data = pca_base_load,
            aes(x = PC1 * 2, y = PC2 * 2, label = rownames(pca_base_load)),
            vjust = 1, hjust = 1, color = "black") +
  stat_chull(fill = NA) +
  geom_label(aes(label = site)) +
  labs(x = paste0("PC", 1, " (", round(pca_base_var[1] * 100, 1), "%)"),
       y = paste0("PC", 2, " (", round(pca_base_var[2] * 100, 1), "%)"))

```

Site 16 splits off from 17 and 18 if there are 4 clusters.

In order to help decide which sites to include from clusters with more than one site, I calculated the euclidean distance between averages of principal components for combinations of 3 sites and the full 5 sites. I only looked at combinations that fit the cluster pattern, so all combinations had to include sites 19 and 20. The table below shows the 16, 17 and 18 compare; a smaller distance values means it is more similar to the full group of sites.
```{r dist mean nc, echo = FALSE}
add15 <- Vectorize(function(str){
  numbers <- str_split(str, ",\\s*")[[1]]
numeric_numbers <- as.numeric(numbers)
new_numbers <- numeric_numbers + 15
output_string <- str_c(new_numbers, collapse = ", ")
return(output_string)
})

combo_dist_base <- evaluate_subsets(scale_base, subset_size = 3) %>% 
  mutate(distance = round(distance, 4),
         indices = add15(indices)) %>% 
  arrange(distance) 

combo_dist <- combo_dist_base %>% 
  filter(
                       str_detect(indices, "19") &
                       str_detect(indices, "20")) %>% 
  head(10)

# index_counts <- combo_dist %>%
#   mutate(indices = str_split(indices, ", ")) %>%
#   unnest(indices) %>%
#   group_by(indices) %>%
#   summarise(count = n(), .groups = 'drop') %>%
#   arrange(desc(count)) %>% 
#   rename(site = indices)

datatable(combo_dist, rownames = FALSE)
```

Assuming that 19 and 20 are being sampled, this tables indicates that 17 is a better fit than 16 or 18.



```{r map 3 nc, echo = FALSE}
prep_base <- dnr %>% 
  filter((site %in% c("16", "17", "18", "19", "20"))) %>% 
               group_by(site) %>% 
               summarise(across(where(is.numeric), ~mean(., na.rm = TRUE))) 
scale_base <- prep_base %>% 
  select(-c(latitude, longitude)) %>% 
               column_to_rownames("site") %>% 
               scale()
pca_base <- prcomp(scale_base)
pca_base_load <- data.frame(pca_base$rotation)
pca_base_var <- (pca_base$sdev^2) / sum(pca_base$sdev^2)
kmean_base <- kmeans(scale_base, 3, nstart = 100)
kmean_base_df <- data.frame(site = names(kmean_base$cluster), cluster = as.numeric(kmean_base$cluster))
```


Here's all 5 sites, colored by the site mean clusters
```{r map base nc, echo = FALSE}

map_data <- left_join(prep_base, kmean_base_df)
pal <- colorFactor(
      palette="viridis",
      domain=map_data$cluster
    )

    leaflet() %>%
      addProviderTiles('Esri.WorldStreetMap') %>%
      addCircleMarkers(
        data = map_data,
        lat = ~latitude,
        lng = ~longitude,
        label = ~str_c(site, ", cluster #", cluster, sep = ""),
        fillColor = ~pal(cluster),
        color="black",
        weight =.5,
        opacity=1,
        fillOpacity=1,
      )

```

### NC Sites Takeaways

I'm not sure how many sites need to be cut to make weekly sampling possible and what is easiest to sample. Based on this analysis, I would drop 18 first, then 16.



